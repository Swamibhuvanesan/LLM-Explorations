# Image Captioner

This project utilizes the `transformers` library to generate captions for given images using a pre-trained model from Huggingface. The specific model used is `Salesforce/blip-image-captioning-base`.

## ğŸ“ Description

The **Image Captioner** project is designed to generate descriptive captions for any given image. This can be particularly useful for tasks involving image recognition, accessibility features, and enhancing user experience in various applications.

## ğŸš€ Getting Started

### Prerequisites

Ensure you have the following dependencies installed:
- Python 3.7 or higher

### Setting Up a Virtual Environment

It is recommended to use a virtual environment to manage your dependencies. You can set up a virtual environment using the following commands:

```sh
# Create a virtual environment
python -m venv venv

# Activate the virtual environment
# On Windows
venv\Scripts\activate
# On macOS/Linux
source venv/bin/activate
```

### Installing Dependencies
You can install the necessary dependencies using pip:
```pip install transformers torch pillow```
# Alternatively, you can use TensorFlow inplace of torch
# pip install tensorflow

###ğŸ“¸ Screenshot
Below is a screenshot of the program in action:
###ğŸ“ Repository Structure
image_captioner.py: The main script to run the image captioning.
psp.jpg: Example image used for captioning.
###ğŸ¤ Contributing
Contributions are welcome! Feel free to fork the repository and submit pull requests. For major changes, please open an issue first to discuss what you would like to change.

###ğŸ“ Connect with Me
I'm always open to new connections and opportunities. Feel free to reach out if you have any questions, suggestions, or just want to connect!

Email: swamibhuvanesan@gmail.com
LinkedIn: Swaminathan
###ğŸŒŸ Acknowledgements
A big thank you to the developers and contributors of the transformers library and the Salesforce/blip-image-captioning-base model.

<p align="center">
  Made with â¤ï¸ by Swaminathan
</p>
